# loader.py

import logging
import os
import shutil
import pandas as pd
from typing import Optional

# Configuration du logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataLoader:
    """
    Classe permettant d'enregistrer un DataFrame PySpark transform√© en fichier CSV.
    Le fichier est nomm√© selon le format "<nom_fichier_base>_processed.csv".
    """

    def __init__(self, spark, output_dir="data\processed_data"):
        """
        Initialise le DataLoader avec un r√©pertoire de sortie.

        :param output_dir: Dossier o√π seront stock√©s les fichiers CSV transform√©s.
        """
        logger.info(
            f"üöÄ Initialisation du DataLoader avec le dossier de sortie : {output_dir}"
        )
        self.spark = spark
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)  # Cr√©e le dossier s'il n'existe pas
        logger.info("‚úÖ Dossier de sortie cr√©√©/valid√©")

    def save_to_csv(self, df, input_file_path):
        """
        Sauvegarde un DataFrame en fichier CSV apr√®s transformation.
        Compatible avec les DataFrames pandas et Spark.
        """
        if df is None:
            logger.error("‚ùå Impossible de sauvegarder un DataFrame vide.")
            return

        # Normaliser le chemin d'entr√©e
        input_file_path = os.path.normpath(input_file_path)
        base_name = os.path.basename(input_file_path).replace(".csv", "_processed.csv")
        final_output_path = os.path.normpath(os.path.join(self.output_dir, base_name))

        logger.info(
            f"‚ö° Enregistrement des donn√©es transform√©es dans : {final_output_path}"
        )

        try:
            # V√©rifier si c'est un DataFrame pandas ou Spark
            if hasattr(df, "toPandas"):  # C'est un DataFrame Spark
                # Utiliser la m√©thode originale pour Spark
                df = df.coalesce(1)
                df.write.mode("overwrite").option("header", "true").option(
                    "delimiter", ";"  # Utiliser la virgule comme s√©parateur
                ).csv(final_output_path + "_temp")

                if temp_file := next(
                    (
                        os.path.join(final_output_path + "_temp", filename)
                        for filename in os.listdir(final_output_path + "_temp")
                        if filename.endswith(".csv")
                    ),
                    None,
                ):
                    shutil.copy2(temp_file, final_output_path)
                    shutil.rmtree(final_output_path + "_temp")
                    logger.info("‚úÖ Fichier CSV sauvegard√© avec succ√®s !")
                else:
                    logger.error(
                        "‚ùå Aucun fichier CSV g√©n√©r√© dans le dossier temporaire."
                    )
            else:  # C'est un DataFrame pandas
                # Pour le fichier de s√©curit√©, on utilise toujours le header
                df.to_csv(final_output_path, sep=";", index=False, header=True)
                logger.info("‚úÖ Fichier CSV sauvegard√© avec succ√®s via pandas!")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'enregistrement du fichier : {str(e)}")
            if os.path.exists(final_output_path + "_temp"):
                shutil.rmtree(final_output_path + "_temp")
                
    def generate_consolidated_csv_from_files(
        self,
        election_csv,    # Chemin vers le CSV des donn√©es politiques
        security_csv,    # Chemin vers le CSV de la s√©curit√©
        socio_csv,       # Chemin vers le CSV de la socio-√©conomie
        sante_csv,       # Chemin vers le CSV de la sant√©
        env_csv,         # Chemin vers le CSV de l‚Äôenvironnement
        edu_csv,         # Chemin vers le CSV de l‚Äô√©ducation
        demo_csv,        # Chemin vers le CSV de la d√©mographie
        tech_csv,        # Chemin vers le CSV de la technologie
        output_filename="consolidated_data.csv"
    ):
        from pyspark.sql.functions import col, lit, concat, lpad, coalesce, first, trim, sum as spark_sum
        from pyspark.sql.types import StringType

        logger.info("üöÄ G√©n√©ration du fichier consolid√© √† partir des CSV...")

        # Liste des d√©partements d√©sir√©s (m√©tropole, sans Corse)
        desired_depts = [
            f"{i:02d}" for i in range(1, 96) if i not in [20]  # exclut le 20 (qui correspond aux codes corse : 2A/2B)
        ]

        # Fichier mapping d√©partements-r√©gions (on suppose que les codes sont d√©j√† format√©s, ex. "01", "02", etc.)
        df_depts = self.spark.read.option("header", "true").csv("data/politique/departements-france.csv") \
            .select(
                trim(col("code_region")).alias("region"),
                col("code_departement").alias("dept")
            ).filter(col("dept").isin(desired_depts))

        # Fichiers d√©partementaux
        df_pol = self.spark.read.option("header", "true").option("delimiter", ";").csv(election_csv) \
            .select(
                col("annee").cast("int"),
                lpad(col("code_dept"), 2, "0").alias("dept"),
                col("id_parti").alias("politique (parti)")
            )

        df_sec = self.spark.read.option("header", "true").option("delimiter", ";").csv(security_csv) \
            .select(
                col("Ann√©e").cast("int").alias("annee"),
                lpad(col("D√©partement"), 2, "0").alias("dept"),
                col("D√©lits_total").alias("securite (Nombre_de_d√©lits)")
            )

        df_sat = self.spark.read.option("header", "true").option("delimiter", ";").csv(sante_csv) \
            .select(
                col("Ann√©e").cast("int").alias("annee"),
                lpad(col("CODE_DEP"), 2, "0").alias("dept"),
                col("Esp√©rance_Vie").alias("sante (Esp√©rance_de_Vie_H/F)")
            )

        # Pour l'√©ducation, on convertit le code en entier puis on le formate en 2 chiffres,
        # on ne garde qu'un enregistrement par (annee, dept)
        target_years = [2002, 2007, 2012, 2017, 2022]
        df_ed = self.spark.read.option("header", "true").option("delimiter", ";").csv(edu_csv) \
            .select(
                col("annee_fermeture").cast("int").alias("annee"),
                lpad(col("code_departement").cast("int").cast("string"), 2, "0").alias("dept"),
                col("nombre_total_etablissements").cast("int").alias("education (Nombre_Total_√âtablissements)")
            ) \
            .filter(col("annee").isin(target_years)) \
            .dropDuplicates(["annee", "dept"])
        # Si plusieurs lignes existent pour un m√™me (annee, dept), on peut agr√©ger avec first(...)

        df_dem = self.spark.read.option("header", "true").option("delimiter", ";").csv(demo_csv) \
            .select(
                col("Ann√©e").cast("int").alias("annee"),
                lpad(col("Code_D√©partement"), 2, "0").alias("dept"),
                col("E_Total").alias("demographie (Population_Totale)")
            )

        df_tech = self.spark.read.option("header", "true").option("delimiter", ";").csv(tech_csv) \
            .select(
                col("annee").cast("int"),
                col("dird_pib_france_pourcentages").alias("technologie (D√©penses_en_R&D_en_pourcentages)")
            )

        # Fichiers r√©gionaux (socio-√©conomie et environnement)
        df_soc = self.spark.read.option("header", "true").option("delimiter", ";").csv(socio_csv) \
            .select(
                coalesce(col("Ann√©e"), col("ann√©e")).cast("int").alias("annee"),
                trim(col("Code_INSEE_R√©gion")).alias("region"),
                col("PIB_par_inflation").alias("socio_economie (PIB_par_Inflation)")
            ) \
            .groupBy("annee", "region") \
            .agg(first("socio_economie (PIB_par_Inflation)").alias("socio_economie (PIB_par_Inflation)")) \
            .join(df_depts, on="region", how="inner") \
            .drop("region")

        df_envr = self.spark.read.option("header", "true").option("delimiter", ";").csv(env_csv) \
            .select(
                coalesce(col("Ann√©e"), col("ann√©e")).cast("int").alias("annee"),
                trim(col("Code_INSEE_R√©gion")).alias("region"),
                col("Parc_install√©_√©olien_MW").alias("environnemental (Parc_install√©_√©olien_MW)")
            ) \
            .groupBy("annee", "region") \
            .agg(first("environnemental (Parc_install√©_√©olien_MW)").alias("environnemental (Parc_install√©_√©olien_MW)")) \
            .join(df_depts, on="region", how="inner") \
            .drop("region")

        # Jointure compl√®te
        df_join = df_pol \
            .join(df_sec, ["annee", "dept"], "full_outer") \
            .join(df_soc, ["annee", "dept"], "full_outer") \
            .join(df_sat, ["annee", "dept"], "full_outer") \
            .join(df_envr, ["annee", "dept"], "full_outer") \
            .join(df_ed, ["annee", "dept"], "left") \
            .join(df_dem, ["annee", "dept"], "full_outer") \
            .join(df_tech, ["annee"], "left")

        # Filtrer uniquement sur les ann√©es pr√©sidentielles et les d√©partements d√©sir√©s
        df_join = df_join.filter(col("annee").isin(target_years) & col("dept").isin(desired_depts))

        # Cl√© finale : concat√©nation de l'ann√©e et du code d√©partement
        df_join = df_join.withColumn("annee_code_dpt", concat(col("annee").cast("string"), lit("_"), col("dept")))

        # S√©lection finale avec les nouveaux noms de colonnes
        df_final = df_join.select(
            "annee_code_dpt",
            "politique (parti)",
            "securite (Nombre_de_d√©lits)",
            "socio_economie (PIB_par_Inflation)",
            "sante (Esp√©rance_de_Vie_H/F)",
            "environnemental (Parc_install√©_√©olien_MW)",
            "education (Nombre_Total_√âtablissements)",
            "demographie (Population_Totale)",
            "technologie (D√©penses_en_R&D_en_pourcentages)"
        ).orderBy("annee_code_dpt")

        logger.info("‚úÖ Donn√©es consolid√©es pr√™tes. Aper√ßu :")
        df_final.show(10, truncate=False)

        self.save_to_csv(df_final, output_filename)