import os
import logging
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    IntegerType,
    StringType,
    DoubleType,
)
from pyspark.sql.functions import col

# Configuration du logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class DataExtractor:
    """
    Classe permettant d'extraire les donn√©es des fichiers CSV pour l'ETL.
    """

    def __init__(self, app_name="EnvironmentalDataETL", master="local[*]"):
        """
        Initialise une session Spark et d√©finit les param√®tres de logging.
        """
        self.spark = (
            SparkSession.builder.appName(app_name)
            .master(master)
            .config("spark.driver.host", "127.0.0.1")
            .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem")
            .getOrCreate()
        )

        # R√©duction des logs Spark pour √©viter le bruit
        self.spark.sparkContext.setLogLevel("ERROR")

    def extract_environmental_data(self, file_path):
        """
        Charge les donn√©es du fichier "parc-regional-annuel-prod-eolien-solaire.csv"
        et les retourne sous forme de DataFrame PySpark.

        :param file_path: Chemin du fichier CSV √† charger
        :return: DataFrame PySpark contenant les donn√©es environnementales
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Fichier non trouv√© : {file_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es environnementales depuis : {file_path}")

        # D√©finition du sch√©ma du fichier
        schema = StructType(
            [
                StructField("Ann√©e", IntegerType(), True),
                StructField("Code INSEE r√©gion", IntegerType(), True),
                StructField("R√©gion", StringType(), True),
                StructField("Parc install√© √©olien (MW)", DoubleType(), True),
                StructField("Parc install√© solaire (MW)", DoubleType(), True),
                StructField("G√©o-shape r√©gion", StringType(), True),
                StructField("G√©o-point r√©gion", StringType(), True),
            ]
        )

        # Chargement du fichier CSV avec gestion des erreurs
        try:
            df = (
                self.spark.read.option("header", "true")
                .option("delimiter", ";")
                .option("enforceSchema", "true")  # Assure que le sch√©ma est respect√©
                .schema(schema)
                .csv(file_path)
            )

            # Normalisation des noms de colonnes (√©vite les erreurs de mapping)
            df = (
                df.withColumnRenamed("Code INSEE r√©gion", "Code_INSEE_r√©gion")
                .withColumnRenamed(
                    "Parc install√© √©olien (MW)", "Parc_install√©_√©olien_MW"
                )
                .withColumnRenamed(
                    "Parc install√© solaire (MW)", "Parc_install√©_solaire_MW"
                )
                .withColumnRenamed("G√©o-shape r√©gion", "G√©o_shape_r√©gion")
                .withColumnRenamed("G√©o-point r√©gion", "G√©o_point_r√©gion")
            )

            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es : {str(e)}")
            return None

    def stop(self):
        """
        Arr√™te la session Spark si elle est active.
        """
        if self.spark:
            self.spark.stop()
            logger.info("üõë Session Spark arr√™t√©e proprement.")
