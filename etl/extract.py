# extract.py

import os
import logging
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    IntegerType,
    StringType,
    DoubleType,
)
from pyspark.sql.functions import col, lit, regexp_extract
import glob

# Configuration du logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class DataExtractor:
    """
    Classe permettant d'extraire les donn√©es des fichiers CSV pour l'ETL.
    """

    def __init__(self, app_name="EnvironmentalDataETL", master="local[*]"):
        """
        Initialise une session Spark et d√©finit les param√®tres de logging.
        """
        self.spark = (
            SparkSession.builder.appName(app_name)
            .master(master)
            .config("spark.driver.host", "127.0.0.1")
            .config(
                "spark.driver.extraClassPath",
                "./database/connector/mysql-connector-j-9.1.0.jar;./database/connector/spark-excel_2.12-3.5.0_0.20.3.jar",
            )
            .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem")
            .config("spark.driver.memory", "8g")
            .config("spark.executor.memory", "8g")
            .config("spark.memory.offHeap.enabled", "true")
            .config("spark.memory.offHeap.size", "8g")
            .getOrCreate()
        )

        # R√©duction des logs Spark pour √©viter le bruit
        self.spark.sparkContext.setLogLevel("ERROR")

    def extract_environmental_data(self, file_path):
        """
        Charge les donn√©es du fichier "parc-regional-annuel-prod-eolien-solaire.csv"
        et les retourne sous forme de DataFrame PySpark.

        :param file_path: Chemin du fichier CSV √† charger
        :return: DataFrame PySpark contenant les donn√©es environnementales
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Fichier non trouv√© : {file_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es environnementales depuis : {file_path}")

        # D√©finition du sch√©ma du fichier
        schema = StructType(
            [
                StructField("Ann√©e", IntegerType(), True),
                StructField("Code INSEE r√©gion", IntegerType(), True),
                StructField("R√©gion", StringType(), True),
                StructField("Parc install√© √©olien (MW)", DoubleType(), True),
                StructField("Parc install√© solaire (MW)", DoubleType(), True),
                StructField("G√©o-shape r√©gion", StringType(), True),
                StructField("G√©o-point r√©gion", StringType(), True),
            ]
        )

        # Chargement du fichier CSV avec gestion des erreurs
        try:
            df = (
                self.spark.read.option("header", "true")
                .option("delimiter", ";")
                .option("enforceSchema", "true")  # Assure que le sch√©ma est respect√©
                .schema(schema)
                .csv(file_path)
            )

            # Normalisation des noms de colonnes (√©vite les erreurs de mapping)
            df = (
                df.withColumnRenamed("Code INSEE r√©gion", "Code_INSEE_r√©gion")
                .withColumnRenamed(
                    "Parc install√© √©olien (MW)", "Parc_install√©_√©olien_MW"
                )
                .withColumnRenamed(
                    "Parc install√© solaire (MW)", "Parc_install√©_solaire_MW"
                )
                .withColumnRenamed("G√©o-shape r√©gion", "G√©o_shape_r√©gion")
                .withColumnRenamed("G√©o-point r√©gion", "G√©o_point_r√©gion")
            )

            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es : {str(e)}")
            return None

    def extract_pib_outre_mer(self, file_paths):
        """
        Extrait et combine les donn√©es PIB brutes des fichiers CSV outre-mer.
        """
        schema = StructType(
            [
                StructField("Ann√©e", StringType(), True),
                StructField("PIB_en_euros_par_habitant", StringType(), True),
                StructField("Codes", StringType(), True),
            ]
        )

        dfs = []
        for path in file_paths:
            if os.path.exists(path):
                df = (
                    self.spark.read.option("header", "true")
                    .option("delimiter", ";")
                    .schema(schema)
                    .csv(path)
                )
                # Ajoute une colonne temporaire indiquant le fichier source
                df = df.withColumn("source_file", lit(path))
                dfs.append(df)
            else:
                logger.error(f"‚ùå Fichier non trouv√© : {path}")

        if not dfs:
            logger.error("‚ùå Aucun fichier PIB valide trouv√©.")
            return None

        from functools import reduce

        df_combined = reduce(lambda df1, df2: df1.union(df2), dfs)

        return df_combined

    def extract_pib_excel(self, excel_path):
        """
        Extrait les donn√©es PIB r√©gionales √† partir du fichier Excel (1990-2021).
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction PIB Excel : {excel_path}")

        schema = StructType(
            [
                StructField("Code_INSEE_R√©gion", StringType(), True),
                StructField("libgeo", StringType(), True),
                StructField("Ann√©e", IntegerType(), True),
                StructField("PIB_en_euros_par_habitant", IntegerType(), True),
            ]
        )

        try:
            df = (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("dataAddress", "'Data'!A5")
                .schema(schema)
                .load(excel_path)
            )

            df = df.select("Ann√©e", "PIB_en_euros_par_habitant", "Code_INSEE_R√©gion")
            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur extraction Excel : {str(e)}")
            return None

    def extract_pib_2022(self, csv_path):
        """
        Extrait et nettoie le fichier PIB 2022 r√©gional CSV.
        """

        if not os.path.exists(csv_path):
            logger.error(f"‚ùå Fichier non trouv√© : {csv_path}")
            return None

        logger.info(f"üì• Extraction PIB 2022 depuis : {csv_path}")

        # D√©finition du sch√©ma correct
        schema = StructType(
            [
                StructField("Code_INSEE_R√©gion", StringType(), True),
                StructField("Libell√©", StringType(), True),
                StructField("PIB_en_euros_par_habitant", IntegerType(), True),
            ]
        )

        # Lecture du fichier en ignorant les 2 premi√®res lignes inutiles
        df_raw = (
            self.spark.read.option("header", "true")
            .option("delimiter", ";")
            .option("inferSchema", "true")  # Permet d'autod√©tecter les types
            .schema(schema)  # Appliquer le sch√©ma explicite
            .csv(csv_path)
        )

        # V√©rification des colonnes d√©tect√©es
        logger.info(f"üõ†Ô∏è Colonnes apr√®s nettoyage : {df_raw.columns}")

        # Ajout de l'ann√©e 2022 √† chaque ligne
        df_cleaned = df_raw.select(
            col("Code_INSEE_R√©gion"),
            lit(2022).alias("Ann√©e"),
            col("PIB_en_euros_par_habitant"),
        )

        return df_cleaned

    def extract_inflation_data(self, excel_path):
        """
        Extrait les donn√©es d'inflation depuis le fichier Excel.

        :param excel_path: Chemin du fichier Excel contenant les donn√©es d'inflation.
        :return: DataFrame PySpark contenant les donn√©es d'inflation filtr√©es de 2000 √† 2022.
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es d'inflation depuis : {excel_path}")

        # D√©finition du sch√©ma explicitement pour correspondre √† la ligne d'en-t√™te effective
        schema = StructType(
            [
                StructField("Ann√©e", IntegerType(), True),
                StructField("√âvolution des prix √† la consommation", DoubleType(), True),
            ]
        )

        try:
            # On sp√©cifie la feuille et la cellule de d√©part (ici A4, suppos√© contenir les en-t√™tes)
            df = (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("sheetName", "Question 1")
                .option("dataAddress", "'Question 1'!A4")
                .schema(schema)
                .load(excel_path)
            )

            # Pour plus de s√©curit√©, renomme la colonne afin de supprimer les espaces
            df = df.withColumnRenamed(
                "√âvolution des prix √† la consommation",
                "√âvolution_des_prix_√†_la_consommation",
            )

            logger.info(f"üõ†Ô∏è Colonnes apr√®s extraction et renommage : {df.columns}")

            # Filtrer les ann√©es de 2000 √† 2022
            df_filtered = df.filter((col("Ann√©e") >= 2000) & (col("Ann√©e") <= 2022))

            logger.info("‚úÖ Extraction des donn√©es d'inflation r√©ussie :")
            df_filtered.show(10, truncate=False)

            return df_filtered

        except Exception as e:
            logger.error(f"‚ùå Erreur extraction Excel inflation : {str(e)}")
            return None

    def extract_technologie_data(self, excel_path):
        """
        Extrait les donn√©es de technologie depuis le fichier Excel.

        :param excel_path: Chemin du fichier Excel contenant les donn√©es de technologie
        :return: DataFrame PySpark contenant les donn√©es brutes
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es de technologie depuis : {excel_path}")

        try:
            df = (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("inferSchema", "true")
                .option("dataAddress", "'Tableau 1'!A3:B37")
                .load(excel_path)
            )

            logger.info("‚úÖ Extraction des donn√©es de technologie r√©ussie")
            return df

        except Exception as e:
            logger.error(
                f"‚ùå Erreur lors de l'extraction des donn√©es de technologie : {str(e)}"
            )
            return None

    def extract_election_data_1965_2012(self, file_pattern):
        """
        Extrait les donn√©es √©lectorales des fichiers CSV de 1965 √† 2012.

        :param file_pattern: Motif des fichiers CSV √† traiter (glob)
        :return: Liste de DataFrames PySpark contenant les donn√©es brutes
        """
        logger.info(f"üì• Extraction des donn√©es √©lectorales 1965-2012 depuis : {file_pattern}")
        file_list = glob.glob(file_pattern)
        results = []

        for file_path in file_list:
            try:
                # Lecture du fichier CSV avec en-t√™te et sch√©ma inf√©r√©
                df = (
                    self.spark.read.option("header", "true")
                    .option("inferSchema", "true")
                    .csv(file_path)
                )

                # Ajout des colonnes ann√©e et nom de fichier
                df = df.withColumn("filename", lit(file_path))
                df = df.withColumn(
                    "annee", regexp_extract("filename", r"presi(\d{4})", 1)
                )

                results.append(df)
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de l'extraction du fichier {file_path}: {str(e)}")
                continue

        if not results:
            logger.warning("Aucun fichier CSV trouv√© pour 1965-2012.")
            return None

        return results

    def extract_election_data_2017(self, excel_file):
        """
        Extrait les donn√©es √©lectorales du fichier Excel 2017.
        """
        logger.info(f"üì• Extraction des donn√©es √©lectorales 2017 : {excel_file}")
        try:
            return (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("inferSchema", "true")
                .option("dataAddress", "'D√©partements Tour 2'!A3:Z115")
                .load(excel_file)
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction 2017 : {str(e)}")
            return None

    def extract_election_data_2022(self, excel_file):
        """
        Extrait les donn√©es √©lectorales du fichier Excel 2022.
        """
        logger.info(f"üì• Extraction des donn√©es √©lectorales 2022 : {excel_file}")
        try:
            return (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("inferSchema", "true")
                .option("sheetName", "R√©sultats")
                .load(excel_file)
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction 2022 : {str(e)}")
            return None

    def stop(self):
        """
        Arr√™te la session Spark si elle est active.
        """
        if self.spark:
            self.spark.stop()
            logger.info("üõë Session Spark arr√™t√©e proprement.")
