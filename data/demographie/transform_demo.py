import os
import pandas as pd
import shutil
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, regexp_replace, expr

# --------------------------------------------------------
# 1Ô∏è‚É£ Conversion du fichier XLS en XLSX avec Pandas
# --------------------------------------------------------

file_xls = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/estim-pop-dep-sexe-gca-1975-2023.xls"
file_xlsx = file_xls.replace(".xls", ".xlsx")

if not os.path.exists(file_xlsx):  # √âviter de convertir plusieurs fois
    print("üì• Conversion du fichier XLS en XLSX...")
    df = pd.read_excel(file_xls, sheet_name=None)  # Charger toutes les feuilles

    with pd.ExcelWriter(file_xlsx, engine="openpyxl") as writer:
        for sheet, data in df.items():
            data.to_excel(writer, sheet_name=sheet, index=False)

    print(f"‚úÖ Conversion r√©ussie : {file_xlsx}")

# --------------------------------------------------------
# 2Ô∏è‚É£ Extraction et fusion des donn√©es dans un seul CSV
# --------------------------------------------------------

file_csv = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_annee.csv"

if not os.path.exists(file_csv):  # √âviter de recr√©er plusieurs fois
    print("üì• Extraction et fusion des feuilles Excel...")

    # Charger toutes les feuilles sauf la premi√®re ("√Ä savoir")
    xls = pd.ExcelFile(file_xlsx)
    sheets_to_read = xls.sheet_names[1:]  # Exclure la premi√®re feuille

    dfs = []
    for sheet in sheets_to_read:
        print(f"üìÑ Traitement de la feuille : {sheet}")
        df = pd.read_excel(xls, sheet_name=sheet, skiprows=3)  # Sauter les 3 premi√®res lignes inutiles
        df["Ann√©e"] = sheet  # Ajouter la colonne Ann√©e
        dfs.append(df)

    df_final = pd.concat(dfs, ignore_index=True)

    # Sauvegarde en CSV
    df_final.to_csv(file_csv, index=False, sep=";")
    print(f"‚úÖ Fichier CSV g√©n√©r√© : {file_csv}")

# --------------------------------------------------------
# 3Ô∏è‚É£ Traitement avec PySpark
# --------------------------------------------------------

# --------------------------------------------------------
# 4Ô∏è‚É£ Traitement avec PySpark
# --------------------------------------------------------

print("üöÄ Lancement du traitement PySpark...")
spark = SparkSession.builder.appName("Traitement_Population").getOrCreate()

df_spark = spark.read.option("header", True).option("sep", ";").csv(file_csv)

# Renommer les colonnes avec les bons en-t√™tes
df_spark = df_spark.withColumnRenamed("0-19 ans", "E_0_19_ans")\
    .withColumnRenamed("20-39 ans", "E_20_39_ans")\
    .withColumnRenamed("40-59 ans", "E_40_59_ans")\
    .withColumnRenamed("60-74 ans", "E_60_74_ans")\
    .withColumnRenamed("75 ans et plus", "E_75_et_plus")\
    .withColumnRenamed("Total", "E_Total")\
    .withColumnRenamed("F_0-19 ans", "F_0_19_ans")\
    .withColumnRenamed("F_20-39 ans", "F_20_39_ans")\
    .withColumnRenamed("F_40-59 ans", "F_40_59_ans")\
    .withColumnRenamed("F_60-74 ans", "F_60_74_ans")\
    .withColumnRenamed("F_75 ans et plus", "F_75_et_plus")\
    .withColumnRenamed("F_Total", "F_Total")\
    .withColumnRenamed("H_0-19 ans", "H_0_19_ans")\
    .withColumnRenamed("H_20-39 ans", "H_20_39_ans")\
    .withColumnRenamed("H_40-59 ans", "H_40_59_ans")\
    .withColumnRenamed("H_60-74 ans", "H_60_74_ans")\
    .withColumnRenamed("H_75 ans et plus", "H_75_et_plus")\
    .withColumnRenamed("H_Total", "H_Total")\
    .withColumnRenamed("Total.1", "Population_Totale")

# Supprimer les lignes de source/notes
df_spark = df_spark.where(
    (~col("Code_D√©partement").startswith("Source")) &
    (~col("Code_D√©partement").startswith("NB:")) &
    (~col("Code_D√©partement").contains("("))
)

# Convertir la colonne Code_D√©partement en num√©rique
df_spark = df_spark.withColumn(
    "Code_D√©partement_Num",
    when(col("Code_D√©partement") == "2A", 201)  # Corse-du-Sud
    .when(col("Code_D√©partement") == "2B", 202)  # Haute-Corse
    .otherwise(col("Code_D√©partement").cast("int"))
)

# R√©organiser les colonnes dans l'ordre souhait√©
df_spark = df_spark.select(
    "Code_D√©partement", "Nom_D√©partement",
    "E_0_19_ans", "E_20_39_ans", "E_40_59_ans", "E_60_74_ans", "E_75_et_plus", "E_Total",
    "F_0_19_ans", "F_20_39_ans", "F_40_59_ans", "F_60_74_ans", "F_75_et_plus", "F_Total",
    "H_0_19_ans", "H_20_39_ans", "H_40_59_ans", "H_60_74_ans", "H_75_et_plus", "H_Total",
    "Ann√©e", "Code_D√©partement_Num"
)

# S√©parer les totaux France M√©tro et DOM-TOM
df_totaux = df_spark.filter(
    (col("Code_D√©partement").contains("France")) | (col("Code_D√©partement").contains("DOM"))
)

# Filtrer les d√©partements uniquement
df_departements = df_spark.subtract(df_totaux)

# Trier par ann√©e (d√©croissant) puis par code d√©partement
df_departements = df_departements.orderBy(col("Ann√©e").desc(), "Code_D√©partement_Num")
df_totaux = df_totaux.orderBy(col("Ann√©e").desc(), "Code_D√©partement_Num")

# üîπ √âcraser les anciens fichiers
dir_departements = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_spark"
dir_totaux = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_totaux_spark"

shutil.rmtree(dir_departements, ignore_errors=True)
shutil.rmtree(dir_totaux, ignore_errors=True)

# √âcrire les fichiers avec les en-t√™tes
df_departements.coalesce(1).write.option("header", True).option("sep", ";").mode("overwrite").csv(dir_departements)
df_totaux.coalesce(1).write.option("header", True).option("sep", ";").mode("overwrite").csv(dir_totaux)

# üîπ Fusion des fichiers en un seul CSV propre üîπ
file_departements = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_spark.csv"
file_totaux = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_totaux_spark.csv"

def overwrite_file(file_path):
    """Supprime le fichier s'il existe d√©j√†"""
    if os.path.exists(file_path):
        os.remove(file_path)

overwrite_file(file_departements)
overwrite_file(file_totaux)

def merge_spark_output(spark_output_dir, output_file):
    """ Fusionne les fichiers Spark en un seul CSV """
    headers = "Code_D√©partement;Nom_D√©partement;E_0_19_ans;E_20_39_ans;E_40_59_ans;E_60_74_ans;E_75_et_plus;E_Total;F_0_19_ans;F_20_39_ans;F_40_59_ans;F_60_74_ans;F_75_et_plus;F_Total;H_0_19_ans;H_20_39_ans;H_40_59_ans;H_60_74_ans;H_75_et_plus;H_Total;Ann√©e;Code_D√©partement_Num\n"
    
    with open(output_file, "w", encoding="utf-8") as outfile:
        # √âcrire les en-t√™tes
        outfile.write(headers)
        
        # Puis √©crire les donn√©es
        for part_file in sorted(os.listdir(spark_output_dir)):
            part_path = os.path.join(spark_output_dir, part_file)
            if part_file.startswith("part-"):  # Ignorer les fichiers _SUCCESS
                with open(part_path, "r", encoding="utf-8") as infile:
                    next(infile)  # Sauter l'en-t√™te du fichier partiel
                    outfile.write(infile.read())

    shutil.rmtree(spark_output_dir)  # Supprime les fichiers interm√©diaires

# Fusionner les r√©sultats
merge_spark_output(dir_departements, file_departements)
merge_spark_output(dir_totaux, file_totaux)

print(f"‚úÖ Traitement termin√© :\n - {file_departements} (donn√©es par d√©partement)\n - {file_totaux} (totaux France)")

# Convertir en pandas pour l'√©criture
df_departements_pd = df_departements.toPandas()
df_totaux_pd = df_totaux.toPandas()

# Cr√©er un dictionnaire de correspondance pour les codes et noms
code_mapping = {
    "France m√©tropolitaine": {"code": "FRM", "nom": "France m√©tropolitaine"},
    "DOM": {"code": "DOM", "nom": "D√©partements d'Outre-Mer"},
    "France m√©tropolitaine et DOM": {"code": "FMD", "nom": "France m√©tropolitaine et DOM"}
}

# Nettoyer les espaces dans la colonne Code_D√©partement
df_totaux_pd["Code_D√©partement"] = df_totaux_pd["Code_D√©partement"].str.strip()

# Ajouter la colonne Type et remplacer les libell√©s par les codes
df_totaux_pd.insert(0, "Type", ["M√©tropole" if x == "France m√©tropolitaine" else "M√©tropole + DOM" if x == "France m√©tropolitaine et DOM" else "DOM" for x in df_totaux_pd["Code_D√©partement"]])

# Mettre √† jour les codes et noms
df_totaux_pd["Nom_D√©partement"] = df_totaux_pd["Code_D√©partement"].map(lambda x: code_mapping[x]["nom"])
df_totaux_pd["Code_D√©partement"] = df_totaux_pd["Code_D√©partement"].map(lambda x: code_mapping[x]["code"])

# √âcrire les fichiers CSV
print("üíæ √âcriture des fichiers CSV...")
df_departements_pd.to_csv(file_departements, sep=";", index=False)
df_totaux_pd.to_csv(file_totaux, sep=";", index=False)

spark.stop()