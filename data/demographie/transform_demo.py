import os
import pandas as pd
import shutil
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, regexp_replace, expr

# --------------------------------------------------------
# 1Ô∏è‚É£ Conversion du fichier XLS en XLSX avec Pandas
# --------------------------------------------------------

file_xls = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/estim-pop-dep-sexe-gca-1975-2023.xls"
file_xlsx = file_xls.replace(".xls", ".xlsx")

if not os.path.exists(file_xlsx):  # √âviter de convertir plusieurs fois
    print("üì• Conversion du fichier XLS en XLSX...")
    df = pd.read_excel(file_xls, sheet_name=None)  # Charger toutes les feuilles

    with pd.ExcelWriter(file_xlsx, engine="openpyxl") as writer:
        for sheet, data in df.items():
            data.to_excel(writer, sheet_name=sheet, index=False)

    print(f"‚úÖ Conversion r√©ussie : {file_xlsx}")

# --------------------------------------------------------
# 2Ô∏è‚É£ Extraction et fusion des donn√©es dans un seul CSV
# --------------------------------------------------------

file_csv = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_annee.csv"

if not os.path.exists(file_csv):  # √âviter de recr√©er plusieurs fois
    print("üì• Extraction et fusion des feuilles Excel...")

    # Charger toutes les feuilles sauf la premi√®re ("√Ä savoir")
    xls = pd.ExcelFile(file_xlsx)
    sheets_to_read = xls.sheet_names[1:]  # Exclure la premi√®re feuille

    dfs = []
    for sheet in sheets_to_read:
        print(f"üìÑ Traitement de la feuille : {sheet}")
        df = pd.read_excel(xls, sheet_name=sheet, skiprows=3)  # Sauter les 3 premi√®res lignes inutiles
        df["Ann√©e"] = sheet  # Ajouter la colonne Ann√©e
        dfs.append(df)

    df_final = pd.concat(dfs, ignore_index=True)

    # Sauvegarde en CSV
    df_final.to_csv(file_csv, index=False, sep=";")
    print(f"‚úÖ Fichier CSV g√©n√©r√© : {file_csv}")

# --------------------------------------------------------
# 3Ô∏è‚É£ Traitement avec PySpark
# --------------------------------------------------------

# --------------------------------------------------------
# 4Ô∏è‚É£ Traitement avec PySpark
# --------------------------------------------------------

print("üöÄ Lancement du traitement PySpark...")
spark = SparkSession.builder.appName("Traitement_Population").getOrCreate()

df_spark = spark.read.option("header", True).option("sep", ";").csv(file_csv)

# Renommer les colonnes avec les bons en-t√™tes
df_spark = df_spark.withColumnRenamed("F 0-19 ans", "F_0_19_ans")\
    .withColumnRenamed("F 20-39 ans", "F_20_39_ans")\
    .withColumnRenamed("F 40-59 ans", "F_40_59_ans")\
    .withColumnRenamed("F 60-74 ans", "F_60_74_ans")\
    .withColumnRenamed("F 75 ans et plus", "F_75_et_plus")\
    .withColumnRenamed("F Total", "F_Total")\
    .withColumnRenamed("H 0-19 ans", "H_0_19_ans")\
    .withColumnRenamed("H 20-39 ans", "H_20_39_ans")\
    .withColumnRenamed("H 40-59 ans", "H_40_59_ans")\
    .withColumnRenamed("H 60-74 ans", "H_60_74_ans")\
    .withColumnRenamed("H 75 ans et plus", "H_75_et_plus")\
    .withColumnRenamed("H Total", "H_Total")\
    .withColumnRenamed("Total", "Population_Totale")

# Supprimer la 2√®me ligne (les intitul√©s en double) et les lignes de source/notes
df_spark = df_spark.where(
    (~col("Code_D√©partement").startswith("Source")) &
    (~col("Code_D√©partement").startswith("NB:")) &
    (~col("Code_D√©partement").contains("("))
)

# Convertir la colonne Code_D√©partement en num√©rique
df_spark = df_spark.withColumn(
    "Code_D√©partement_Num",
    when(col("Code_D√©partement") == "2A", 201)  # Corse-du-Sud
    .when(col("Code_D√©partement") == "2B", 202)  # Haute-Corse
    .otherwise(col("Code_D√©partement").cast("int"))
)

# Trier par ann√©e (d√©croissant) puis par code d√©partement
df_spark = df_spark.orderBy(col("Ann√©e").desc(), "Code_D√©partement_Num")

# S√©parer les totaux France M√©tro et DOM-TOM
df_totaux = df_spark.filter(
    (col("Code_D√©partement").contains("France")) | (col("Code_D√©partement").contains("DOM"))
)

# Filtrer les d√©partements uniquement
df_departements = df_spark.subtract(df_totaux)

# üîπ √âcraser les anciens fichiers
dir_departements = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_spark"
dir_totaux = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_totaux_spark"

shutil.rmtree(dir_departements, ignore_errors=True)
shutil.rmtree(dir_totaux, ignore_errors=True)

df_departements.write.option("header", True).option("sep", ";").mode("overwrite").csv(dir_departements)
df_totaux.write.option("header", True).option("sep", ";").mode("overwrite").csv(dir_totaux)

# üîπ Fusion des fichiers en un seul CSV propre üîπ
file_departements = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_spark.csv"
file_totaux = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_totaux_spark.csv"

def overwrite_file(file_path):
    """Supprime le fichier s'il existe d√©j√†"""
    if os.path.exists(file_path):
        os.remove(file_path)

overwrite_file(file_departements)
overwrite_file(file_totaux)

def merge_spark_output(spark_output_dir, output_file):
    """ Fusionne les fichiers Spark en un seul CSV """
    with open(output_file, "w", encoding="utf-8") as outfile:
        for i, part_file in enumerate(sorted(os.listdir(spark_output_dir))):
            part_path = os.path.join(spark_output_dir, part_file)
            if part_file.startswith("part-"):  # Ignorer les fichiers _SUCCESS
                with open(part_path, "r", encoding="utf-8") as infile:
                    if i == 0:  # Garde l'en-t√™te du premier fichier uniquement
                        outfile.write(infile.read())
                    else:
                        infile.readline()  # Supprime la premi√®re ligne (header)
                        outfile.write(infile.read())

    shutil.rmtree(spark_output_dir)  # Supprime les fichiers interm√©diaires

# Fusionner les r√©sultats
merge_spark_output(dir_departements, file_departements)
merge_spark_output(dir_totaux, file_totaux)

print(f"‚úÖ Traitement termin√© :\n - {file_departements} (donn√©es par d√©partement)\n - {file_totaux} (totaux France)")

spark.stop()