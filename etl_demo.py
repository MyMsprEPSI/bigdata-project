import os
import shutil
import subprocess
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, trim, lower, lit, year
from pyspark.sql.types import IntegerType

# ========================
# 1. Conversion XLS -> XLSX
# ========================
def convert_excel_to_xlsx(file_xls, file_xlsx):
    """
    Convertit le fichier XLS en XLSX si ce dernier n'existe pas d√©j√†.
    """
    if not os.path.exists(file_xlsx):
        print("üì• Conversion du fichier XLS en XLSX...")
        try:
            # Charger toutes les feuilles du fichier XLS
            excel_data = pd.read_excel(file_xls, sheet_name=None)
            with pd.ExcelWriter(file_xlsx, engine="openpyxl") as writer:
                for sheet, data in excel_data.items():
                    data.to_excel(writer, sheet_name=sheet, index=False)
            print(f"‚úÖ Conversion r√©ussie : {file_xlsx}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la conversion : {e}")
    else:
        print(f"‚úì {file_xlsx} existe d√©j√†. Conversion ignor√©e.")

# ================================
# 2. Extraction et fusion des donn√©es Excel en CSV
# ================================
def extract_and_merge_excel(file_xlsx, file_csv):
    """
    Extrait et fusionne les feuilles Excel (hors premi√®re feuille) en un unique CSV.
    """
    if not os.path.exists(file_csv):
        print("üì• Extraction et fusion des feuilles Excel...")
        try:
            # Charger le classeur XLSX
            xls = pd.ExcelFile(file_xlsx)
            # On ignore la premi√®re feuille (ex: "√Ä savoir")
            sheets_to_read = xls.sheet_names[1:]
            dfs = []
            for sheet in sheets_to_read:
                print(f"üìÑ Traitement de la feuille : {sheet}")
                df = pd.read_excel(xls, sheet_name=sheet, skiprows=3)
                df["Ann√©e"] = sheet  # Ajouter l'ann√©e issue du nom de la feuille
                dfs.append(df)
            df_final = pd.concat(dfs, ignore_index=True)
            # Sauvegarder en CSV avec le s√©parateur point-virgule
            df_final.to_csv(file_csv, index=False, sep=";")
            print(f"‚úÖ Fichier CSV g√©n√©r√© : {file_csv}")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction/fusion : {e}")
    else:
        print(f"‚úì {file_csv} existe d√©j√†. Extraction ignor√©e.")

# ========================
# 3. Traitement avec PySpark
# ========================
def transform_demographie_data(df):
    """
    Effectue le renommage des colonnes, filtre et r√©organise les donn√©es d√©mographie.
    """
    # Renommage initial des colonnes principales
    df = df.withColumnRenamed("D√©partements", "Code_D√©partement") \
           .withColumnRenamed("Unnamed: 1", "Nom_D√©partement") \
           .withColumnRenamed("Ensemble", "E_Total") \
           .withColumnRenamed("Hommes", "H_Total") \
           .withColumnRenamed("Femmes", "F_Total")
    
    # Renommage des colonnes pour les tranches d'√¢ge
    # Pour Ensemble (E)
    df = df.withColumnRenamed("Unnamed: 3", "E_0_19_ans") \
           .withColumnRenamed("Unnamed: 4", "E_20_39_ans") \
           .withColumnRenamed("Unnamed: 5", "E_40_59_ans") \
           .withColumnRenamed("Unnamed: 6", "E_60_74_ans") \
           .withColumnRenamed("Unnamed: 7", "E_75_et_plus")
    
    # Pour Hommes (H)
    df = df.withColumnRenamed("Unnamed: 9", "H_0_19_ans") \
           .withColumnRenamed("Unnamed: 10", "H_20_39_ans") \
           .withColumnRenamed("Unnamed: 11", "H_40_59_ans") \
           .withColumnRenamed("Unnamed: 12", "H_60_74_ans") \
           .withColumnRenamed("Unnamed: 13", "H_75_et_plus")
    
    # Pour Femmes (F)
    df = df.withColumnRenamed("Unnamed: 15", "F_0_19_ans") \
           .withColumnRenamed("Unnamed: 16", "F_20_39_ans") \
           .withColumnRenamed("Unnamed: 17", "F_40_59_ans") \
           .withColumnRenamed("Unnamed: 18", "F_60_74_ans") \
           .withColumnRenamed("Unnamed: 19", "F_75_et_plus")
    
    # Suppression des lignes de source/notes
    df = df.filter(
        (~col("Code_D√©partement").startswith("Source")) &
        (~col("Code_D√©partement").startswith("NB:")) &
        (~col("Code_D√©partement").contains("("))
    )
    
    # Ajout d'une colonne num√©rique pour le tri uniquement
    df = df.withColumn("Code_D√©partement_Num",
                      col("Code_D√©partement").cast(IntegerType()))
    
    # R√©organisation des colonnes dans l'ordre souhait√©
    df = df.select(
        "Code_D√©partement", "Nom_D√©partement",
        "E_0_19_ans", "E_20_39_ans", "E_40_59_ans", "E_60_74_ans", "E_75_et_plus", "E_Total",
        "F_0_19_ans", "F_20_39_ans", "F_40_59_ans", "F_60_74_ans", "F_75_et_plus", "F_Total",
        "H_0_19_ans", "H_20_39_ans", "H_40_59_ans", "H_60_74_ans", "H_75_et_plus", "H_Total",
        "Ann√©e", "Code_D√©partement_Num"
    )
    
    # Tri par ann√©e d√©croissante puis par code d√©partement
    df = df.orderBy(col("Ann√©e").desc(), "Code_D√©partement_Num")
    
    return df

def separate_totals(df):
    """
    S√©pare les totaux (France M√©tropolitaine et DOM-TOM) des donn√©es d√©partementales.
    """
    df_totaux = df.filter(
        col("Code_D√©partement").contains("France") | col("Code_D√©partement").contains("DOM")
    )
    df_departements = df.subtract(df_totaux)
    # R√©ordonner chacun
    df_totaux = df_totaux.orderBy(col("Ann√©e").desc(), "Code_D√©partement_Num")
    df_departements = df_departements.orderBy(col("Ann√©e").desc(), "Code_D√©partement_Num")
    return df_totaux, df_departements

# ========================
# 4. √âcriture des sorties Spark et fusion
# ========================
def write_spark_output(df, output_folder):
    """
    √âcrit le DataFrame Spark dans un dossier en for√ßant la sortie en un seul fichier CSV.
    """
    if os.path.exists(output_folder):
        shutil.rmtree(output_folder)
    try:
        (df.coalesce(1)
           .write
           .mode("overwrite")
           .option("header", True)
           .option("sep", ";")
           .csv(output_folder))
        print(f"‚úì Donn√©es √©crites dans {output_folder}")
    except Exception as e:
        print(f"‚ùå Erreur lors de l'√©criture dans {output_folder} : {e}")

def merge_spark_output(spark_output_dir, output_file):
    """
    Fusionne les fichiers CSV g√©n√©r√©s par Spark en un seul fichier CSV.
    """
    # Si le fichier de sortie existe d√©j√†, on le supprime
    if os.path.exists(output_file):
        os.remove(output_file)
    
    # Le header √† √©crire est fix√© selon l'ordre des colonnes
    headers = "Code_D√©partement;Nom_D√©partement;E_0_19_ans;E_20_39_ans;E_40_59_ans;E_60_74_ans;E_75_et_plus;E_Total;" \
              "F_0_19_ans;F_20_39_ans;F_40_59_ans;F_60_74_ans;F_75_et_plus;F_Total;" \
              "H_0_19_ans;H_20_39_ans;H_40_59_ans;H_60_74_ans;H_75_et_plus;H_Total;Ann√©e;Code_D√©partement_Num\n"
    
    with open(output_file, "w", encoding="utf-8") as outfile:
        outfile.write(headers)
        for part_file in sorted(os.listdir(spark_output_dir)):
            part_path = os.path.join(spark_output_dir, part_file)
            if part_file.startswith("part-") and part_file.endswith(".csv"):
                with open(part_path, "r", encoding="utf-8") as infile:
                    next(infile)  # Sauter le header du fichier partiel
                    outfile.write(infile.read())
    # Supprimer le dossier interm√©diaire
    shutil.rmtree(spark_output_dir)
    print(f"‚úì Fichier fusionn√© cr√©√© : {output_file}")

# ========================
# 5. Nettoyage final avec Pandas
# ========================
def final_cleaning(file_totaux, file_departements):
    """
    Effectue un dernier nettoyage sur le CSV des totaux en appliquant une correspondance pour les codes et libell√©s.
    Puis, r√©√©crit les fichiers finaux.
    """
    try:
        # Chargement des CSV en DataFrame Pandas
        df_totaux_pd = pd.read_csv(file_totaux, sep=";")
        df_departements_pd = pd.read_csv(file_departements, sep=";")
        
        # Dictionnaire de correspondance
        code_mapping = {
            "France m√©tropolitaine": {"code": "FRM", "nom": "France m√©tropolitaine"},
            "DOM": {"code": "DOM", "nom": "D√©partements d'Outre-Mer"},
            "France m√©tropolitaine et DOM": {"code": "FMD", "nom": "France m√©tropolitaine et DOM"}
        }
        
        # Nettoyage des espaces et ajout de la colonne Type
        df_totaux_pd["Code_D√©partement"] = df_totaux_pd["Code_D√©partement"].str.strip()
        df_totaux_pd.insert(0, "Type", df_totaux_pd["Code_D√©partement"].apply(
            lambda x: "M√©tropole" if x == "France m√©tropolitaine" 
            else "M√©tropole + DOM" if x == "France m√©tropolitaine et DOM" 
            else "DOM"))
        
        # Mise √† jour des codes et noms avec le mapping
        df_totaux_pd["Nom_D√©partement"] = df_totaux_pd["Code_D√©partement"].map(lambda x: code_mapping.get(x, {}).get("nom", x))
        df_totaux_pd["Code_D√©partement"] = df_totaux_pd["Code_D√©partement"].map(lambda x: code_mapping.get(x, {}).get("code", x))
        
        # R√©√©criture des fichiers CSV finaux
        df_totaux_pd.to_csv(file_totaux, sep=";", index=False)
        df_departements_pd.to_csv(file_departements, sep=";", index=False)
        print("‚úÖ Fichiers finaux √©crits avec succ√®s.")
    except Exception as e:
        print(f"‚ùå Erreur lors du nettoyage final : {e}")

# ========================
# Fonction principale
# ========================
def main():
    # Configuration de Java et Spark
    os.environ['JAVA_HOME'] = r'C:\Program Files\Java\jdk-17'
    os.environ['SPARK_HOME'] = r'C:\Users\thoma\AppData\Roaming\Python\Python312\site-packages\pyspark'
    os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\bin;' + os.environ['SPARK_HOME'] + r'\bin;' + os.environ['PATH']
    os.environ['PYSPARK_PYTHON'] = r'C:\Python312\python.exe'
    os.environ['PYSPARK_DRIVER_PYTHON'] = r'C:\Python312\python.exe'
    
    # V√©rification de Java
    try:
        subprocess.run(['java', '-version'], capture_output=True, check=True)
    except FileNotFoundError:
        print("Erreur: Java n'est pas trouv√©. Veuillez installer Java 17 et configurer JAVA_HOME.")
        return

    # Chemins des fichiers
    file_xls = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/estim-pop-dep-sexe-gca-1975-2023.xls"
    file_xlsx = file_xls.replace(".xls", ".xlsx")
    file_csv = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_annee.csv"
    
    # Conversion et extraction/fusion des donn√©es Excel
    convert_excel_to_xlsx(file_xls, file_xlsx)
    extract_and_merge_excel(file_xlsx, file_csv)
    
    # Lancement de Spark
    print("üöÄ Lancement du traitement PySpark...")
    spark = SparkSession.builder \
            .appName("Traitement_Demographie") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.extraJavaOptions", "-Dfile.encoding=UTF-8") \
            .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
            .master("local[*]") \
            .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    
    # Lecture du CSV fusionn√©
    df_spark = spark.read.option("header", True).option("sep", ";").csv(file_csv)
    
    # Transformation des donn√©es d√©mographie
    df_spark = transform_demographie_data(df_spark)
    
    # S√©paration des totaux et des d√©partements
    df_totaux, df_departements = separate_totals(df_spark)
    
    # D√©finition des dossiers et fichiers de sortie
    dir_departements = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_spark"
    dir_totaux = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_totaux_spark"
    file_departements = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_par_departement_spark.csv"
    file_totaux = "D:/Thomas/Documents/GitHub/bigdata-project/data/demographie/population_totaux_spark.csv"
    
    # √âcriture des sorties Spark
    write_spark_output(df_departements, dir_departements)
    write_spark_output(df_totaux, dir_totaux)
    
    # Fusion des sorties en un seul fichier CSV par ensemble
    merge_spark_output(dir_departements, file_departements)
    merge_spark_output(dir_totaux, file_totaux)
    
    # Nettoyage final avec Pandas
    final_cleaning(file_totaux, file_departements)
    
    # Arr√™t de la session Spark
    spark.stop()
    print("üöÄ Traitement termin√©.")

if __name__ == "__main__":
    main()
